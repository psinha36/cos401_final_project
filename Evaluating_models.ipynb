{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using ROUGE and BERT scores to evaluate model's generated summaries"
      ],
      "metadata": {
        "id": "H5sbHYUmk3dl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nmyJnRfDkhUK"
      },
      "outputs": [],
      "source": [
        "! pip --quiet install transformers datasets evaluate rouge_score torch bert_score py7zr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "nST_lQAwlVXB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, logout\n",
        "\n",
        "login(\"<>\", add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "wsYIM-93k2F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "old_books_data_train = load_dataset(\"psin/old_books_data_train\")\n",
        "old_books_data_test = load_dataset(\"psin/old_books_data_test\")\n",
        "new_books_data_train = load_dataset(\"psin/new_books_data_train\")\n",
        "new_books_data_test = load_dataset(\"psin/new_books_data_test\")\n"
      ],
      "metadata": {
        "id": "Tesxa-hXlDqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_modern_text = [text for text in new_books_data_test['train']['text']]\n",
        "all_modern_summaries = [text for text in new_books_data_test['train']['summary']]"
      ],
      "metadata": {
        "id": "Kj8LUi-OmA8o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = load_metric(\"bertscore\")"
      ],
      "metadata": {
        "id": "9Qa6ryC8mL09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred, decoded=False):\n",
        "    predictions, labels = eval_pred\n",
        "    if decoded:\n",
        "       decoded_preds = predictions\n",
        "       decoded_labels = labels\n",
        "    else:\n",
        "      decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    \n",
        "    # paper reccomended rescaling baseline \n",
        "    bert_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\", rescale_with_baseline=True)\n",
        "    bert_result_final = bert_result\n",
        "    print(bert_result.keys())\n",
        "    bert_result_final['precision'] = np.round(np.mean(bert_result['precision']), 4)\n",
        "    bert_result_final['recall'] = np.round(np.mean(bert_result['recall']), 4)\n",
        "    bert_result_final['f1'] = np.round(np.mean(bert_result['f1']))\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    rouge_result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    rouge_result_final = {k: round(v, 4) for k, v in rouge_result.items()}\n",
        "    \n",
        "    final_result = rouge_result_final | bert_result_final\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "i5vSnzBemY7_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "def get_prediction(text, model, tokenizer, n_gram_block=4, num_beams=3):\n",
        "  tokenized_inputs = tokenizer(text, max_length=512, return_tensors=\"pt\").input_ids\n",
        "  output = model.generate(tokenized_inputs, max_new_tokens=200, do_sample=False, no_repeat_ngram_size=n_gram_block, num_beams=num_beams)\n",
        "  return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "YLGCJ1yuoh84"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_modern_text[:3])\n",
        "for checkpoint in ['psin/xsum_and_billsum_and_samsum_old']:\n",
        "  print(f\"Testing model: {checkpoint}\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "  predicted_text_list = []\n",
        "  for text in all_modern_text:\n",
        "    pred = get_prediction(text, model, tokenizer, n_gram_block=4, num_beams=3)\n",
        "    predicted_text_list.append(pred)\n",
        "  print(compute_metrics((predicted_text_list, all_modern_summaries), decoded=True))"
      ],
      "metadata": {
        "id": "gcc2aShopQSi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}