{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loading your datasets and fine-tuning models"
      ],
      "metadata": {
        "id": "unKqCDusGcty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip --quiet install transformers datasets evaluate rouge_score torch bert_score py7zr"
      ],
      "metadata": {
        "id": "6RGCtoByGajt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log into hugging face to access private datasets and to push trained models"
      ],
      "metadata": {
        "id": "O74ypJc2GRqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, logout\n",
        "\n",
        "login(\"<access token>\", add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "LHjhtA9vGQx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load necessary datasets"
      ],
      "metadata": {
        "id": "dSG3bnAzGFRh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMJs-o6DGBeE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "xsum = load_dataset(\"xsum\")\n",
        "billsum = load_dataset(\"billsum\")\n",
        "samsum = load_dataset(\"samsum\")\n",
        "\n",
        "# private repos\n",
        "old_books_data_train = load_dataset(\"psin/old_books_data_train\")\n",
        "old_books_data_test = load_dataset(\"psin/old_books_data_test\")\n",
        "new_books_data_train = load_dataset(\"psin/new_books_data_train\")\n",
        "new_books_data_test = load_dataset(\"psin/new_books_data_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the dataset to see how texts and summaries are tagged. Remove any irrelevant columns\n",
        "\n"
      ],
      "metadata": {
        "id": "i3K2vZAPHArh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(xsum)\n",
        "print(billsum)\n",
        "print(samsum)\n",
        "print(old_books_data_train)"
      ],
      "metadata": {
        "id": "g2STJEGPG8U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removes extraneous columsn"
      ],
      "metadata": {
        "id": "xL5Yshi-HQAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xsum = xsum.remove_columns(['id'])\n",
        "billsum = billsum.remove_columns(['title'])\n",
        "samsum.remove_columns('id')"
      ],
      "metadata": {
        "id": "shTUe7SUHTNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get pre-trained T5 tokenizer"
      ],
      "metadata": {
        "id": "PHUv546HHwJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "LvCGzG3PHvlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define functions for pre-processing the data. Pre-pend the texts with the \"summarize: \" prefix."
      ],
      "metadata": {
        "id": "ZuSBEAwOH-to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function_fiction(examples, text_column_name=\"text\", summary_column_name=\"summary\", prefix = \"summarize: \"):\n",
        "    # already prefixed summary tag\n",
        "    inputs = [doc for doc in examples[text_column_name]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[summary_column_name], max_length=256, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "Y1pm27e1H27S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function_data(examples, text_column_name=\"text\", summary_column_name=\"summary\", prefix = \"summarize: \"):\n",
        "    inputs = [prefix + doc for doc in examples[text_column_name]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    labels = tokenizer(text_target=examples[summary_column_name], max_length=256, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "Vg0L5vu2H7Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the pre-processing function to the data "
      ],
      "metadata": {
        "id": "1eCzLPK1IXbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_billsum = billsum.map(preprocess_function_billsum, batched=True)"
      ],
      "metadata": {
        "id": "PRpmxPmBIerD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the relevant evaluation metrics"
      ],
      "metadata": {
        "id": "747-bjLWIpKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = load_metric(\"bertscore\")"
      ],
      "metadata": {
        "id": "Ww_OoXZNIrJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to compute metrics and return them as a dictionary. Average values over the batch."
      ],
      "metadata": {
        "id": "ou4UCq5XIvcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred, decoded=False):\n",
        "    predictions, labels = eval_pred\n",
        "    if decoded:\n",
        "       decoded_preds = predictions\n",
        "       decoded_labels = labels\n",
        "    else:\n",
        "      decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    \n",
        "    # paper reccomended rescaling baseline \n",
        "    bert_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\", rescale_with_baseline=True)\n",
        "    bert_result_final = bert_result\n",
        "    print(bert_result.keys())\n",
        "    bert_result_final['precision'] = np.round(np.mean(bert_result['precision']), 4)\n",
        "    bert_result_final['recall'] = np.round(np.mean(bert_result['recall']), 4)\n",
        "    bert_result_final['f1'] = np.round(np.mean(bert_result['f1']))\n",
        "\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    rouge_result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    rouge_result_final = {k: round(v, 4) for k, v in rouge_result.items()}\n",
        "    \n",
        "    \n",
        "    \n",
        "    final_result = rouge_result_final | bert_result_final\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "6ZufGav8IytA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the base model"
      ],
      "metadata": {
        "id": "EASvk_7SJAKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model_checkpoint = \"t5-small\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "MJya38DFI8Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data collator"
      ],
      "metadata": {
        "id": "kRChOwDfJFlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "checkpoint = \"t5-small\"\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
      ],
      "metadata": {
        "id": "YtYVzvSlJHub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "_sGAKEOmJRhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"model name\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=5,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    seed = 72,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_billsum['train'],\n",
        "    eval_dataset=tokenized_billsum['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "xK6HwP0VJSmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If pushing large files"
      ],
      "metadata": {
        "id": "3hc7Xh7-Jodl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "Yzjb19orJlZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "id": "Y7ABOUpgJn0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Push the model to your account"
      ],
      "metadata": {
        "id": "URHjEZCzJhDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()\n"
      ],
      "metadata": {
        "id": "9Hw68vClJgmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}